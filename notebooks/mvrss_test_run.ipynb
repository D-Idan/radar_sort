{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/daniel/Idan/University/Masters/Thesis/2024/radar_sort')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add package root to the path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "path_root = Path().resolve().parents[0]\n",
    "sys.path.append(str(path_root))\n",
    "path_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.paths_collector import Paths\n",
    "from data.carrada.dataset import Carrada\n",
    "from evaluation.tester import Tester\n",
    "from mvrss.utils.functions import count_params\n",
    "# from mvrss.learners.tester import Tester\n",
    "from mvrss.models import TMVANet, MVNet\n",
    "# from mvrss.loaders.dataset import Carrada\n",
    "from mvrss.loaders.dataloaders import SequenceCarradaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "args = {\n",
    "    'cfg': '/Users/daniel/Idan/University/Masters/Thesis/2024/datasets/logs/carrada/mvnet/mvnet_e300_lr0.0001_s42_0/config.json',\n",
    "}\n",
    "\n",
    "target_seq = '2019-09-16-12-55-51' # None for all\n",
    "frame_num_plot = 400\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    cfg_path = args[\"cfg\"]\n",
    "    with open(cfg_path, 'r') as fp:\n",
    "        cfg = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    paths = Paths().get()\n",
    "\n",
    "    exp_name = cfg['name_exp'] + '_' + str(cfg['version'])\n",
    "    path = paths['logs'] / cfg['dataset'] / cfg['model'] / exp_name\n",
    "    model_path = path / 'results' / 'model.pt'\n",
    "    test_results_path = path / 'results' / 'test_results.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'warehouse': PosixPath('/Users/daniel/Idan/University/Masters/Thesis/2024/datasets'),\n",
       " 'logs': PosixPath('/Users/daniel/Idan/University/Masters/Thesis/2024/datasets/logs'),\n",
       " 'carrada': PosixPath('/Users/daniel/Idan/University/Masters/Thesis/2024/datasets/Carrada'),\n",
       " 'config': '/Users/daniel/Idan/University/Masters/Thesis/2024/radar_sort/configs/config.ini'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in the model: 2375432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wh/181dmx3d2v94m8cyv4ny1s_w0000gn/T/ipykernel_83636/2230332469.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n"
     ]
    }
   ],
   "source": [
    "    if cfg['model'] == 'mvnet':\n",
    "        model = MVNet(n_classes=cfg['nb_classes'], n_frames=cfg['nb_input_channels'])\n",
    "    else:\n",
    "        model = TMVANet(n_classes=cfg['nb_classes'], n_frames=cfg['nb_input_channels'])\n",
    "    print('Number of trainable parameters in the model: %s' % str(count_params(model)))\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = Tester(cfg)\n",
    "\n",
    "data = Carrada(config_model=cfg)\n",
    "test = data.get('Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the target sequence if specified\n",
    "if target_seq is not None:\n",
    "    test = {target_seq: test[target_seq]}\n",
    "    \n",
    "testset = SequenceCarradaDataset(test)\n",
    "seq_testloader = DataLoader(\n",
    "    testset,\n",
    "    batch_size=1,\n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.set_annot_type(cfg['annot_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMethod to predict on a given dataset using a fixed model\\n\\nPARAMETERS\\n----------\\nnet: PyTorch Model\\n    Network to test\\nseq_loader: DataLoader\\n    Specific to the dataset used for test\\niteration: int\\n    Iteration used to display visualization\\n    Default: None\\nget_quali: boolean\\n    If you want to save qualitative results\\n    Default: False\\nadd_temp: boolean\\n    Is the data are considered as a sequence\\n    Default: False\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Method to predict on a given dataset using a fixed model\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "net: PyTorch Model\n",
    "    Network to test\n",
    "seq_loader: DataLoader\n",
    "    Specific to the dataset used for test\n",
    "iteration: int\n",
    "    Iteration used to display visualization\n",
    "    Default: None\n",
    "get_quali: boolean\n",
    "    If you want to save qualitative results\n",
    "    Default: False\n",
    "add_temp: boolean\n",
    "    Is the data are considered as a sequence\n",
    "    Default: False\n",
    "\"\"\"\n",
    "        \n",
    "# if cfg['model'] == 'mvnet':\n",
    "#     test_results = tester.predict(model, seq_testloader, get_quali=True, add_temp=False)\n",
    "# else:\n",
    "#     test_results = tester.predict(model, seq_testloader, get_quali=True, add_temp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester.write_params(test_results_path)\n",
    "# test_results_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Method Walkthrough\n",
    "This notebook breaks down the `predict` method of the Tester class step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Initial Setup\n",
    "First we need all required imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from mvrss.utils.functions import transform_masks_viz, get_metrics, normalize, define_loss, get_transformations, get_qualitatives\n",
    "# from mvrss.utils.paths import Paths\n",
    "from mvrss.utils.metrics import Evaluator\n",
    "from mvrss.loaders.dataloaders import CarradaDataset\n",
    "\n",
    "from utils.paths_collector import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, seq_loader = model, seq_testloader\n",
    "iteration=None\n",
    "get_quali=True\n",
    "add_temp=True\n",
    "\n",
    "visualizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "        cfg = cfg\n",
    "        visualizer = visualizer\n",
    "        model = cfg['model']\n",
    "        nb_classes = cfg['nb_classes']\n",
    "        annot_type = cfg['annot_type']\n",
    "        process_signal = cfg['process_signal']\n",
    "        w_size = cfg['w_size']\n",
    "        h_size = cfg['h_size']\n",
    "        n_frames = cfg['nb_input_channels']\n",
    "        batch_size = cfg['batch_size']\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        custom_loss = cfg['custom_loss']\n",
    "        transform_names = cfg['transformations'].split(',')\n",
    "        norm_type = cfg['norm_type']\n",
    "        paths = Paths().get()\n",
    "        test_results = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Preparing the Evaluation Environment\n",
    "In this cell, we set the network to evaluation mode, create the test-time transformations, initialize loss functions and evaluation metrics, and define some lists to collect running losses. We also set a random sequence index if you are using visualization for a specific iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg  # Set the network to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "# Prepare transformations for the test split (assumes get_transformations is defined)\n",
    "transformations = get_transformations(\n",
    "    transform_names, split='test', sizes=(w_size, h_size))\n",
    "\n",
    "# Define loss functions for range_doppler and range_angle (assumes define_loss is defined)\n",
    "rd_criterion = define_loss('range_doppler', custom_loss, device)\n",
    "ra_criterion = define_loss('range_angle', custom_loss, device)\n",
    "nb_losses = len(rd_criterion)\n",
    "\n",
    "# Initialize lists to store running losses and detailed sub-losses\n",
    "running_losses = []\n",
    "rd_running_losses = []\n",
    "rd_running_global_losses = [[], []]  # Two sub-losses for range_doppler\n",
    "ra_running_losses = []\n",
    "ra_running_global_losses = [[], []]  # Two sub-losses for range_angle\n",
    "coherence_running_losses = []\n",
    "\n",
    "# Initialize evaluators for metrics (assumes Evaluator is defined)\n",
    "rd_metrics = Evaluator(num_class=nb_classes)\n",
    "ra_metrics = Evaluator(num_class=nb_classes)\n",
    "\n",
    "# If an iteration number is provided, choose a random sequence from the loader for visualization\n",
    "if iteration is not None:\n",
    "    rand_seq = np.random.randint(len(seq_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping Over Sequences\n",
    "Here we iterate over the sequences in the seq_loader. For each sequence, we create a frame-level dataloader and prepare for potential visualization if the iteration flag is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each sequence in the seq_loader\n",
    "for i, sequence_data in enumerate(seq_loader):\n",
    "    seq_name, seq = sequence_data\n",
    "    # Construct the path to the frames (assumes paths is defined with key 'carrada')\n",
    "    path_to_frames = paths['carrada'] / seq_name[0]\n",
    "    \n",
    "    # Create a DataLoader for the current sequence frames (assumes CarradaDataset is defined)\n",
    "    frame_dataloader = DataLoader(\n",
    "        CarradaDataset(seq, annot_type, path_to_frames, process_signal, n_frames, transformations, add_temp),\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    # If visualization is enabled and we are on the random sequence, choose a random frame index\n",
    "    if iteration is not None and i == rand_seq:\n",
    "        rand_frame = np.random.randint(len(frame_dataloader))\n",
    "    \n",
    "    # Set qualitative iteration counters (for visualization) if qualitative results are needed\n",
    "    if get_quali:\n",
    "        quali_iter_rd = n_frames - 1\n",
    "        quali_iter_ra = n_frames - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Each Frame\n",
    "Within each sequence, we loop over the frames. For each frame we:\n",
    "\n",
    "-    Load and normalize the data (range-doppler, range-angle, and optionally angle-doppler).\n",
    "\n",
    "-    Pass the data through the network.\n",
    "-    Compute the predictions and optionally save qualitative results.\n",
    "-   Update the evaluation metrics.\n",
    "-   Compute and store the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frame_num_plot from the dataloader\n",
    "for frame_num, frame in enumerate(frame_dataloader):\n",
    "    if frame_num == frame_num_plot:\n",
    "        break\n",
    "# Retrieve tensors and move them to the device (assumes proper keys exist in frame)\n",
    "rd_data = frame['rd_matrix'].to(device).float()\n",
    "ra_data = frame['ra_matrix'].to(device).float()\n",
    "ad_data = frame['ad_matrix'].to(device).float()  # Only used if model is 'tmvanet'\n",
    "rd_mask = frame['rd_mask'].to(device).float()\n",
    "ra_mask = frame['ra_mask'].to(device).float()\n",
    "\n",
    "# Normalize the inputs (assumes normalize is defined)\n",
    "rd_data = normalize(rd_data, 'range_doppler', norm_type=norm_type).squeeze()\n",
    "ra_data = normalize(ra_data, 'range_angle', norm_type=norm_type).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the network (check if model requires angle_doppler data)\n",
    "if model == 'tmvanet':\n",
    "    ad_data = normalize(ad_data, 'angle_doppler', norm_type=norm_type).squeeze()\n",
    "    rd_outputs, ra_outputs = net(rd_data, ra_data, ad_data)\n",
    "else:\n",
    "    rd_outputs, ra_outputs = net(rd_data, ra_data)\n",
    "\n",
    "# Ensure outputs are on the proper device\n",
    "rd_outputs = rd_outputs.to(device)\n",
    "ra_outputs = ra_outputs.to(device)\n",
    "ra_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Results\n",
    "Finally, we plot the results using the `plot_results` method. This method takes the collected losses and metrics and generates visualizations to help us understand the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 260, 332])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_pred_masks = torch.argmax(rd_outputs, axis=1)[:5]\n",
    "ra_pred_masks = torch.argmax(ra_outputs, axis=1)[:5]\n",
    "rd_gt_masks = torch.argmax(rd_mask, axis=1)[:5]\n",
    "ra_gt_masks = torch.argmax(ra_mask, axis=1)[:5]\n",
    "rd_pred_grid = make_grid(transform_masks_viz(rd_pred_masks,\n",
    "                                                nb_classes))\n",
    "ra_pred_grid = make_grid(transform_masks_viz(ra_pred_masks,\n",
    "                                                nb_classes))\n",
    "rd_gt_grid = make_grid(transform_masks_viz(rd_gt_masks,\n",
    "                                            nb_classes))\n",
    "ra_gt_grid = make_grid(transform_masks_viz(ra_gt_masks,\n",
    "                                            nb_classes))\n",
    "# visualizer.update_multi_img_masks(rd_pred_grid, rd_gt_grid,\n",
    "#                                         ra_pred_grid, ra_gt_grid,\n",
    "#                                         iteration)\n",
    "rd_gt_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rd_gt_grid max value: 0.0. \n",
      " if this is 0, then the image is black\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming rd_gt_grid is a PyTorch tensor of shape [3, 260, 332]\n",
    "rgb_image = rgb_image.cpu()\n",
    "# 1. Permute to [H, W, C] (matplotlib expects height × width × channels)\n",
    "rgb_image = rd_gt_grid.permute(1, 2, 0)  # Now shape [260, 332, 3]\n",
    "\n",
    "# 2. Normalize if needed (if values are not in [0, 1] or [0, 255])\n",
    "if torch.max(rgb_image) > 1.0:\n",
    "    rgb_image = rgb_image / rgb_image.max()  # Scale to [0, 1]\n",
    "\n",
    "# 3. Plot\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.imshow(rgb_image)  # No need for cmap in RGB mode\n",
    "plt.title('RGB Image')\n",
    "plt.axis('off')  # Hide axes\n",
    "\n",
    "print(f'rd_gt_grid max value: {torch.max(rd_gt_grid)}. \\n if this is 0, then the image is black')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(rd_pred_grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.imshow(rd_gt_grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # Loop over frames in the current sequence\n",
    "    # for j, frame in enumerate(frame_dataloader):\n",
    "    #     # Retrieve tensors and move them to the device (assumes proper keys exist in frame)\n",
    "    #     rd_data = frame['rd_matrix'].to(device).float()\n",
    "    #     ra_data = frame['ra_matrix'].to(device).float()\n",
    "    #     ad_data = frame['ad_matrix'].to(device).float()  # Only used if model is 'tmvanet'\n",
    "    #     rd_mask = frame['rd_mask'].to(device).float()\n",
    "    #     ra_mask = frame['ra_mask'].to(device).float()\n",
    "        \n",
    "    #     # Normalize the inputs (assumes normalize is defined)\n",
    "    #     rd_data = normalize(rd_data, 'range_doppler', norm_type=norm_type).squeeze()\n",
    "    #     ra_data = normalize(ra_data, 'range_angle', norm_type=norm_type).squeeze()\n",
    "        \n",
    "    #     # Forward pass through the network (check if model requires angle_doppler data)\n",
    "    #     if model == 'tmvanet':\n",
    "    #         ad_data = normalize(ad_data, 'angle_doppler', norm_type=norm_type).squeeze()\n",
    "    #         rd_outputs, ra_outputs = net(rd_data, ra_data, ad_data)\n",
    "    #     else:\n",
    "    #         rd_outputs, ra_outputs = net(rd_data, ra_data)\n",
    "        \n",
    "    #     # Ensure outputs are on the proper device\n",
    "    #     rd_outputs = rd_outputs.to(device)\n",
    "    #     ra_outputs = ra_outputs.to(device)\n",
    "        \n",
    "    #     # Optional: save qualitative results if flag is enabled (assumes get_qualitatives is defined)\n",
    "    #     if get_quali:\n",
    "    #         quali_iter_rd = get_qualitatives(rd_outputs, rd_mask, paths, seq_name, quali_iter_rd, 'range_doppler')\n",
    "    #         quali_iter_ra = get_qualitatives(ra_outputs, ra_mask, paths, seq_name, quali_iter_ra, 'range_angle')\n",
    "        \n",
    "    #     # Update evaluation metrics (assumes torch.argmax and Evaluator.add_batch are defined)\n",
    "    #     rd_metrics.add_batch(torch.argmax(rd_mask, axis=1).cpu(),\n",
    "    #                          torch.argmax(rd_outputs, axis=1).cpu())\n",
    "    #     ra_metrics.add_batch(torch.argmax(ra_mask, axis=1).cpu(),\n",
    "    #                          torch.argmax(ra_outputs, axis=1).cpu())\n",
    "        \n",
    "    #     # Compute losses depending on the number of loss functions\n",
    "    #     if nb_losses < 3:\n",
    "    #         # Case without the Coherence Loss (CoL)\n",
    "    #         rd_losses = [c(rd_outputs, torch.argmax(rd_mask, axis=1)) for c in rd_criterion]\n",
    "    #         rd_loss = torch.mean(torch.stack(rd_losses))\n",
    "    #         ra_losses = [c(ra_outputs, torch.argmax(ra_mask, axis=1)) for c in ra_criterion]\n",
    "    #         ra_loss = torch.mean(torch.stack(ra_losses))\n",
    "    #         loss = torch.mean(rd_loss + ra_loss)\n",
    "    #     else:\n",
    "    #         # Case with the Coherence Loss (CoL)\n",
    "    #         rd_losses = [c(rd_outputs, torch.argmax(rd_mask, axis=1)) for c in rd_criterion[:2]]\n",
    "    #         rd_loss = torch.mean(torch.stack(rd_losses))\n",
    "    #         ra_losses = [c(ra_outputs, torch.argmax(ra_mask, axis=1)) for c in ra_criterion[:2]]\n",
    "    #         ra_loss = torch.mean(torch.stack(ra_losses))\n",
    "    #         # Compute the coherence loss between rd_outputs and ra_outputs\n",
    "    #         coherence_loss = rd_criterion[2](rd_outputs, ra_outputs)\n",
    "    #         loss = torch.mean(rd_loss + ra_loss + coherence_loss)\n",
    "        \n",
    "    #     # Store the computed losses (use .data.cpu().numpy() to convert tensors to numpy values)\n",
    "    #     running_losses.append(loss.data.cpu().numpy()[()])\n",
    "    #     rd_running_losses.append(rd_loss.data.cpu().numpy()[()])\n",
    "    #     rd_running_global_losses[0].append(rd_losses[0].data.cpu().numpy()[()])\n",
    "    #     rd_running_global_losses[1].append(rd_losses[1].data.cpu().numpy()[()])\n",
    "    #     ra_running_losses.append(ra_loss.data.cpu().numpy()[()])\n",
    "    #     ra_running_global_losses[0].append(ra_losses[0].data.cpu().numpy()[()])\n",
    "    #     ra_running_global_losses[1].append(ra_losses[1].data.cpu().numpy()[()])\n",
    "    #     if nb_losses > 2:\n",
    "    #         coherence_running_losses.append(coherence_loss.data.cpu().numpy()[()])\n",
    "        \n",
    "    #     # If this is the selected random sequence and frame, update the visualizations\n",
    "    #     if iteration is not None and i == rand_seq:\n",
    "    #         if j == rand_frame:\n",
    "    #             # Get the predicted masks for visualization (take only first 5 for brevity)\n",
    "    #             rd_pred_masks = torch.argmax(rd_outputs, axis=1)[:5]\n",
    "    #             ra_pred_masks = torch.argmax(ra_outputs, axis=1)[:5]\n",
    "    #             rd_gt_masks = torch.argmax(rd_mask, axis=1)[:5]\n",
    "    #             ra_gt_masks = torch.argmax(ra_mask, axis=1)[:5]\n",
    "    #             # Convert masks to grids for visualization (assumes make_grid and transform_masks_viz are defined)\n",
    "    #             rd_pred_grid = make_grid(transform_masks_viz(rd_pred_masks, nb_classes))\n",
    "    #             ra_pred_grid = make_grid(transform_masks_viz(ra_pred_masks, nb_classes))\n",
    "    #             rd_gt_grid = make_grid(transform_masks_viz(rd_gt_masks, nb_classes))\n",
    "    #             ra_gt_grid = make_grid(transform_masks_viz(ra_gt_masks, nb_classes))\n",
    "    #             # Update visualization (assumes visualizer.update_multi_img_masks is defined)\n",
    "    #             visualizer.update_multi_img_masks(rd_pred_grid, rd_gt_grid,\n",
    "    #                                               ra_pred_grid, ra_gt_grid,\n",
    "    #                                               iteration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating and Reporting the Results\n",
    "After processing all sequences and frames, we aggregate the computed metrics and losses into a results dictionary. Finally, we reset the metric evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "{'range_doppler': {'loss': nan, 'loss_ce': nan, 'loss_dice': nan, 'acc': np.float64(0.0), 'acc_by_class': [0.0, 0.0, 0.0, 0.0], 'prec': np.float64(0.0), 'prec_by_class': [0.0, 0.0, 0.0, 0.0], 'recall': np.float64(0.0), 'recall_by_class': [0.0, 0.0, 0.0, 0.0], 'miou': np.float64(0.0), 'miou_by_class': [0.0, 0.0, 0.0, 0.0], 'dice': np.float64(0.0), 'dice_by_class': [0.0, 0.0, 0.0, 0.0]}, 'range_angle': {'loss': nan, 'loss_ce': nan, 'loss_dice': nan, 'acc': np.float64(0.0), 'acc_by_class': [0.0, 0.0, 0.0, 0.0], 'prec': np.float64(0.0), 'prec_by_class': [0.0, 0.0, 0.0, 0.0], 'recall': np.float64(0.0), 'recall_by_class': [0.0, 0.0, 0.0, 0.0], 'miou': np.float64(0.0), 'miou_by_class': [0.0, 0.0, 0.0, 0.0], 'dice': np.float64(0.0), 'dice_by_class': [0.0, 0.0, 0.0, 0.0]}, 'global_acc': np.float64(0.0), 'global_prec': np.float64(0.0), 'global_dice': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/miniconda3/envs/radar_sort/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/daniel/miniconda3/envs/radar_sort/lib/python3.11/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/daniel/Idan/University/Masters/Thesis/2024/radar_sort/external/MVRSS/mvrss/utils/metrics.py:41: RuntimeWarning: invalid value encountered in divide\n",
      "  acc_by_class = np.diag(self.confusion_matrix).sum() / (np.nansum(self.confusion_matrix, axis=1)\n",
      "/Users/daniel/Idan/University/Masters/Thesis/2024/radar_sort/external/MVRSS/mvrss/utils/metrics.py:21: RuntimeWarning: invalid value encountered in divide\n",
      "  prec_by_class = np.diag(self.confusion_matrix) / np.nansum(self.confusion_matrix, axis=0)\n",
      "/Users/daniel/Idan/University/Masters/Thesis/2024/radar_sort/external/MVRSS/mvrss/utils/metrics.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  recall_by_class = np.diag(self.confusion_matrix) / np.nansum(self.confusion_matrix, axis=1)\n",
      "/Users/daniel/Idan/University/Masters/Thesis/2024/radar_sort/external/MVRSS/mvrss/utils/metrics.py:54: RuntimeWarning: invalid value encountered in divide\n",
      "  miou_by_class = np.diag(self.confusion_matrix) / (np.nansum(self.confusion_matrix, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# After processing all sequences, compute the final test results\n",
    "test_results = dict()\n",
    "\n",
    "# Compute metrics for both range_doppler and range_angle (assumes get_metrics is defined)\n",
    "test_results['range_doppler'] = get_metrics(\n",
    "    rd_metrics,\n",
    "    np.mean(rd_running_losses),\n",
    "    [np.mean(sub_loss) for sub_loss in rd_running_global_losses]\n",
    ")\n",
    "test_results['range_angle'] = get_metrics(\n",
    "    ra_metrics,\n",
    "    np.mean(ra_running_losses),\n",
    "    [np.mean(sub_loss) for sub_loss in ra_running_global_losses]\n",
    ")\n",
    "\n",
    "# If coherence loss was computed, include it in the results\n",
    "if nb_losses > 2:\n",
    "    test_results['coherence_loss'] = np.mean(coherence_running_losses).item()\n",
    "\n",
    "# Compute global metrics by averaging the two views\n",
    "test_results['global_acc'] = 0.5 * \\\n",
    "    (test_results['range_doppler']['acc'] + test_results['range_angle']['acc'])\n",
    "test_results['global_prec'] = 0.5 * \\\n",
    "    (test_results['range_doppler']['prec'] +\n",
    "     test_results['range_angle']['prec'])\n",
    "test_results['global_dice'] = 0.5 * \\\n",
    "    (test_results['range_doppler']['dice'] +\n",
    "     test_results['range_angle']['dice'])\n",
    "\n",
    "# Reset the metrics for potential further evaluations\n",
    "rd_metrics.reset()\n",
    "ra_metrics.reset()\n",
    "\n",
    "# Display the final test results\n",
    "print(\"Test Results:\")\n",
    "print(test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radar_sort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
